// ğŸ¤– Ollama AI æœåŠ¡

export interface OllamaModel {
  name: string;
  size: number;
  digest: string;
  modified_at: string;
  details?: {
    format: string;
    family: string;
    families: string[];
    parameter_size: string;
    quantization_level: string;
  };
  isThinkingModel?: boolean;
}

export interface OllamaResponse {
  model: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  eval_count?: number;
  eval_duration?: number;
}

export class OllamaService {
  private baseUrl: string;
  private timeout: number;

  constructor(baseUrl: string = 'http://localhost:11434', timeout: number = 30000) {
    this.baseUrl = baseUrl.replace(/\/$/, ''); // ç§»é™¤å°¾éƒ¨æ–œæ 
    this.timeout = timeout;
  }

  // === æ¨¡å‹ç®¡ç† ===

  /**
   * è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async getModels(): Promise<OllamaModel[]> {
    try {
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(this.timeout)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const data = await response.json();
      const models = data.models || [];

      // è¯†åˆ«æ€è€ƒæ¨¡å‹å¹¶æ ‡è®°
      return models.map((model: any) => ({
        ...model,
        isThinkingModel: this.isThinkingModel(model.name)
      }));
    } catch (error) {
      console.error('è·å–Ollamaæ¨¡å‹åˆ—è¡¨å¤±è´¥:', error);
      throw new Error(`æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ (${this.baseUrl}): ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    }
  }

  /**
   * æ£€æµ‹æ˜¯å¦ä¸ºæ€è€ƒæ¨¡å‹
   */
  private isThinkingModel(modelName: string): boolean {
    const thinkingKeywords = [
      'thinking', 'reason', 'analysis', 'cot', 'chain-of-thought',
      'qwen2.5-coder', 'deepseek', 'o1', 'reasoning'
    ];
    
    const name = modelName.toLowerCase();
    return thinkingKeywords.some(keyword => name.includes(keyword));
  }

  /**
   * æµ‹è¯•æ¨¡å‹è¿æ¥
   */
  async testModel(modelName: string): Promise<{ success: boolean; response?: string; error?: string }> {
    try {
      const testPrompt = this.getTestPrompt(modelName);
      const response = await this.generateResponse(modelName, testPrompt);
      
      return {
        success: true,
        response: response.response
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'
      };
    }
  }

  /**
   * æ ¹æ®æ¨¡å‹ç±»å‹ç”Ÿæˆæµ‹è¯•æç¤ºè¯
   */
  private getTestPrompt(modelName: string): string {
    if (this.isThinkingModel(modelName)) {
      return `è¯·ç®€å•å›ç­”ï¼š1+1ç­‰äºå¤šå°‘ï¼Ÿè¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼Œä¸éœ€è¦è¯¦ç»†æ¨ç†è¿‡ç¨‹ã€‚`;
    } else {
      return `ä½ å¥½ï¼è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ï¼Œå‘Šè¯‰æˆ‘ä½ æ˜¯ä»€ä¹ˆAIæ¨¡å‹ã€‚`;
    }
  }

  // === å¯¹è¯ç”Ÿæˆ ===

  /**
   * ç”ŸæˆAIå›å¤
   */
  async generateResponse(
    modelName: string, 
    prompt: string, 
    options: {
      temperature?: number;
      top_p?: number;
      max_tokens?: number;
      context?: number[];
    } = {}
  ): Promise<OllamaResponse> {
    try {
      const isThinking = this.isThinkingModel(modelName);
      const adaptedPrompt = this.adaptPromptForModel(prompt, isThinking);

      const requestBody = {
        model: modelName,
        prompt: adaptedPrompt,
        stream: false,
        options: {
          temperature: options.temperature ?? 0.7,
          top_p: options.top_p ?? 0.9,
          num_predict: options.max_tokens ?? 2048,
          ...options
        },
        context: options.context
      };

      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(this.timeout)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const result = await response.json();
      return result;
    } catch (error) {
      console.error('Ollamaç”Ÿæˆå›å¤å¤±è´¥:', error);
      throw new Error(`ç”Ÿæˆå›å¤å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    }
  }

  /**
   * æ ¹æ®æ¨¡å‹ç±»å‹é€‚é…æç¤ºè¯
   */
  private adaptPromptForModel(prompt: string, isThinkingModel: boolean): string {
    if (isThinkingModel) {
      // æ€è€ƒæ¨¡å‹ï¼šé¼“åŠ±è¯¦ç»†æ€è€ƒè¿‡ç¨‹
      return `è¯·ä»”ç»†æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼Œä½ å¯ä»¥å±•ç¤ºä½ çš„æ€è€ƒè¿‡ç¨‹ï¼š

${prompt}

è¯·æä¾›è¯¦ç»†çš„åˆ†æå’Œæ¨ç†è¿‡ç¨‹ã€‚`;
    } else {
      // æ™®é€šæ¨¡å‹ï¼šç®€æ´ç›´æ¥çš„å›ç­”
      return `è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¿æŒå›ç­”ç®€æ´æ˜äº†ï¼š

${prompt}`;
    }
  }

  // === æµå¼å“åº” ===

  /**
   * æµå¼ç”Ÿæˆå“åº”
   */
  async *generateStreamResponse(
    modelName: string,
    prompt: string,
    options: {
      temperature?: number;
      top_p?: number;
      max_tokens?: number;
      context?: number[];
    } = {}
  ): AsyncGenerator<string, void, unknown> {
    try {
      const isThinking = this.isThinkingModel(modelName);
      const adaptedPrompt = this.adaptPromptForModel(prompt, isThinking);

      const requestBody = {
        model: modelName,
        prompt: adaptedPrompt,
        stream: true,
        options: {
          temperature: options.temperature ?? 0.7,
          top_p: options.top_p ?? 0.9,
          num_predict: options.max_tokens ?? 2048,
        },
        context: options.context
      };

      const response = await fetch(`${this.baseUrl}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(requestBody),
        signal: AbortSignal.timeout(this.timeout)
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      const reader = response.body?.getReader();
      if (!reader) {
        throw new Error('æ— æ³•è·å–å“åº”æµ');
      }

      const decoder = new TextDecoder();
      let buffer = '';

      try {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop() || '';

          for (const line of lines) {
            if (line.trim()) {
              try {
                const data = JSON.parse(line);
                if (data.response) {
                  yield data.response;
                }
                if (data.done) {
                  return;
                }
              } catch (e) {
                console.warn('è§£ææµæ•°æ®å¤±è´¥:', line);
              }
            }
          }
        }
      } finally {
        reader.releaseLock();
      }
    } catch (error) {
      console.error('Ollamaæµå¼ç”Ÿæˆå¤±è´¥:', error);
      throw new Error(`æµå¼ç”Ÿæˆå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}`);
    }
  }

  // === å·¥å…·æ–¹æ³• ===

  /**
   * æ£€æŸ¥æœåŠ¡è¿æ¥çŠ¶æ€
   */
  async checkConnection(): Promise<{ connected: boolean; version?: string; error?: string }> {
    try {
      const response = await fetch(`${this.baseUrl}/api/version`, {
        method: 'GET',
        signal: AbortSignal.timeout(5000)
      });

      if (response.ok) {
        const version = await response.json();
        return {
          connected: true,
          version: version.version
        };
      } else {
        return {
          connected: false,
          error: `HTTP ${response.status}: ${response.statusText}`
        };
      }
    } catch (error) {
      return {
        connected: false,
        error: error instanceof Error ? error.message : 'è¿æ¥å¤±è´¥'
      };
    }
  }

  /**
   * æ›´æ–°åŸºç¡€URL
   */
  updateBaseUrl(newUrl: string): void {
    this.baseUrl = newUrl.replace(/\/$/, '');
  }

  /**
   * è·å–å½“å‰é…ç½®
   */
  getConfig(): { baseUrl: string; timeout: number } {
    return {
      baseUrl: this.baseUrl,
      timeout: this.timeout
    };
  }
} 